# Joint Distributions

Let's start with an example:
$X, Y$: Bernoulli,

|            | Y = 0 | Y = 1  | 
|    ---     |      :-:       | :-: |
| **X = 0**  | $\frac{2}{6}$  | $\frac{1}{6}$ |
| **X = 1**  | $\frac{2}{6}$  | $\frac{1}{6}$ |

**Condition**: The numbers in the table have to be non-negative _and_ add up to 1.
Here, $\frac{2}{6} + \frac{2}{6} + \frac{1}{6} + \frac{1}{6} = 1 $

### Definitions
Assume $X, Y$ are random variables:   
- PMF: Probability Mass Function, 
- PDF: Probability Density Function (measure of probability),
- CDF: Cumulative Distribution Function
 
##### For Discrete Random Variables
**Joint CDF**: $F(x, y) = P(X <= x, Y <= y)$
**Joint PMF**: $f(x, y) = P(X = x, Y = y)$

**Joint CDF**: $F(x) = P(X <= x)$
**Marginal PMF**: $F(x) = P(X = x)$

##### For Continuous Random Variables
**Joint PDF** is $f(x, y)$ such that:
$P((x, y) \in B) = {\int\int}_B f(x, y) dx dy $
Where $B$ is some region in the plane.


### Independence 
##### Discrete
For all  $x, y \in \mathbb{R}$:
- **CDF-Wise**: $X, Y$ are independent iff   
$$P(X <= x, Y <= y) = P(X <= x) \times F(Y <= y)$$
- **PMF-Wise**: $X, Y$ are independent iff    
$$P(X = x, Y = y) = P(X = x) \times P(Y = y)$$
##### Continuous
- **PDF-Wise**: $X, Y$ are independent iff   
$$f(x, y) = f_X(x) \times f_Y(y) $$


### Getting Marginals
##### Discrete
$P(X = x) = \sum_y P(X = x, Y = y)$

##### Continuous
$P(X = x) = \int_{-\infty}^{\infty} f(x, y) dy$


### Example #1
In the joint probability table provided above, are $X$ and $Y$ independent?

**Marginals**:
$P(X = 0) = \frac{2}{6} + \frac{1}{6} = \frac{1}{2}$
$P(X = 1) = \frac{2}{6} + \frac{1}{6} = \frac{1}{2}$
$P(Y = 0) = \frac{2}{6} + \frac{2}{6} = \frac{2}{3}$
$P(Y = 1) = \frac{1}{6} + \frac{1}{6} = \frac{1}{3}$

**Test**:
$P(X = 0, Y = 0) = \frac{2}{6}$
$P(X=0) \times P(Y=0) = \frac{1}{2} \times \frac{2}{3} = \frac{2}{6}$
=> (same goes for all other numbers) $X$ and $Y$ are independent.


### Example #2
$X, Y$: Bernoulli,

|            | Y = 0 | Y = 1  | 
|    ---     |      :-:       | :-: |
| **X = 0**  | $\frac{1}{2}$  | $0$ |
| **X = 1**  | $\frac{1}{4}$  | $\frac{1}{4}$ |

$P(X = 0) = \frac{1}{2} + 0 = \frac{1}{2}$
$P(X = 1) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}$
$P(Y = 0) = \frac{1}{4} + \frac{1}{2} = \frac{3}{4}$
$P(Y = 1) = \frac{1}{4} + 0 = \frac{1}{4}$

$P(X = 0, Y = 0) = \frac{1}{2} \neq P(X=0) \times P(Y=0) = \frac{3}{8}$
=> $X$ and $Y$ are not independent.

### Example #3 (continuous, independent)
Assume a uniform distribution on a square $(x, y): x, y \in [0, 1]$
![](./.pics/uniform-x1y1.png)

The distribution is uniform, therefore the _joint PDF_ is constant (**c**) inside the square, and $0$ outside of it.
$\int_{square} f(x, y) = \int_{square} c = c \times area |^{x=1}_{x=0} \space |^{y=1}_{y=0} = c \times 1 = 1 $

Therefore,
$$
c = 
\begin{cases}
  1 & 0 \leq x \leq 1, & 0 \leq y \leq 1 \\
  0 & \text{otherwise}
\end{cases}
$$

$X$ and $Y$ are independent uniform, which is pretty intuitive as well. Meaning that if you pick any point in the square, both the $X$ and $Y$ will be uniformly random.

### Example #4 (continuous, not independent)
Assume a uniform distribution inside a unit circle $(x, y): x^2 + y^2 = 1$
The distribution is uniform, therefore the _joint PDF_ is constant (**c**) inside the circle, and $0$ outside of it.

$\int_{circle} f(x, y) = \int_{circle} c = c \times \pi r^2 |^{r=1}_{r=0} = c \times \pi = 1 $

Therefore,
$$
c = 
\begin{cases}
  \frac{1}{\pi} & x^2 + y^2 = 1 \\
  0 & \text{otherwise}
\end{cases}
$$

Note that in this equation's condition, $x$ and $y$ are not independent (unlike the previous example).
e.g. if $x = 0.95$, $y$ will have a much smaller allowed range of values than when $x = 0$. 
i.e. Given $X=x$, $-\sqrt{1-x^2} \leq Y \leq \sqrt{1-x^2}$
