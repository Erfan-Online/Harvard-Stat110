## 19. Joint, Conditional, and Marginal Distributions

**Joint PDF**: $F(x, y) = P(X \leq x, Y \leq y)$
Continuous Case (**Joint PDF**): $f(x, y) = \frac{\partial^2}{\partial x \partial y} F(x, y)$   
Which also means: $P((x, y) \in A) = {\int \int}_A \space f(x, y) dx dy$

**Marginal PDF of X**: $\int^{+\infty}_{-\infty} f(x, y)dy$
Which also means: $\int^{+\infty}_{-\infty} \int^{+\infty}_{-\infty} f(x, y)dy dx = 1$

**Conditional PDF of $Y|X$**:
$$ f_{Y|X} (y|x) = \frac{f_{X,Y}(x, y)}{f_X(x)} = \frac{f_{X|Y}(x|y)f_Y(y)}{f_X(x)} $$

Think of is as the PDF where we pretend to know what $X$ is. 
i.e. Given that we know the value of $X$, what is the appropriate PDF for $Y$?

The proof for the first equality comes from conditional probability:
Say $Y$ is extremely close to $y$ (assuming probability isn't $0$), then find the conditional probability of that, given the value of $X$.
And the proof of second equality is basically **Bayes' rule**, taken at the limit.