## Random Variables
(Start from the second part of lecture 6 handwritten notes)

A **Random Variable** is a function from the _Sample Space_ $S$ to $\mathbb{R}$.
We can think of it as a numerical "summary" of an aspect of our experiments. 
[Our experiments are our source of randomness]

Essentially, thye're a mapping from the outputs of an experiment to real numbers.

### Probability Mass Function (PMF)
The PMF defines the probabilities for the events in a **discrete** sample space, examples of PMF are the $P(X=k)$ given in the below distributions, where $[X=k]$ is the event.

In other words, assume our output can take on the following values: $a_1, a_2, ...$ (finite or infinite).
Then, the PMF is:
$$
P(x = a_j) = p_j \text{ for all } j, \text{where } p_j \geq 0, \text{ and } \sum_{j=1}^n p_j = 1.
$$

### Cumulative Distribution Function (CDF)
The CDF is the probability that a random variable is less than or equal to a given value.
The event is $[X \leq x]$, and the _CDF_ is 
$$ 
F(x) = P(X \leq x)
$$

### Bernoulli Distribution
A random variable $X$ is said to have a $Bern(p)$ distribution if $X$ has only $2$ possible values, $0$ and $1$.
_PMF_:
$$P(X=1) = p, \\  P(X=0) = 1 - p = q$$

$[X=1]$ is an event $\{S : X(S) = 1 \}$: which includes all outputs which are assigned to the number $1$ by th random variable.

### Binomial Distribution
The distribution of **\#successes** in $n$ independent $Bern(p)$ experiments is called the $Bin(n, p)$.
_PMF_:
$$
P(X=k) = {n \choose k} p^k (1-p)^{n - k} \quad \text{for \,} k \in [0, 1, ... n]
$$

**Proof**:
For example consider the result $111000$, the probability of this happening is $p^k(1-p)^{n-k}$, but all permutations with $k$ number of $1$s are equivalent in this regard.

The binomial distribution can also be considered as a sum of indicator random variables:
$$ 
X = X_1 + X_2 + ... + X_n \, , \quad X_j = 
\begin{cases}
  1 & \text{if $j$-th trial is a success} \\
  0 & \text{otherwise} 
\end{cases}
$$

Where $X_1, ..., X_n$ are independent and identically distributed (**iid**) random variables.

And just to check the consistency of the above with the definition of PMF:
$$
\sum_{k=0}^n {n \choose k} p^k (1-p)^{n-k} \stackrel{\text{Binomial theorem}}{=} (p + (1-p))^n = 1^n = 1
$$

_(Fairly Obvious) Note_: Given $X \sim Bin(n, p)$, $Y \sim Bin(m , p)$, we can say:
$$ X + Y \sim Bin(n + m, p) $$

**(different) Proofs**:
1. Immediate from story.
2. Sum of indicator random variables: 
    $$
    X = X_1 + X_2 + ... + X_n \, , \quad Y = Y_1 + Y_2 + ... + Y_m \\
    \Rightarrow X + Y = \sum_{i=1}^n X_i + \sum_{j=1}^m Y_j 
    $$
    Which is the sum of $(n+m)$ iid $\sim Bern(p)$.

3. PMF: 
    $$
    \begin{align*}
    P(X+Y = k) &\stackrel{LOTP}{=} \sum_{j=0}^n P(X+Y=k | X=j) P(X=j) \\
    &\stackrel{Y+j=k}{=} \sum_{j=0}^{k} p(Y=k-j | X=j) {n \choose j} p^j(1-p)^{n-j} \\
    &\stackrel{Y=k-j \text{ and } X=j \text{ are independent}}{=} \sum_{j=0}^k p(Y=k-j) {n \choose j} p^j(1-p)^{n-j} \\
    &= \sum_{j=0}^k {m \choose k-j} p^{k-j} (1-p)^{m-k+j} \times {n \choose j} p^j(1-p)^{n-j} \\
    &= p^k (1-p)^{m+n-k} \sum_{j=0}^k {m \choose k-j} {n \choose j} \\
    &\stackrel{\text{Vandermonde's identity}}{=} p^k(1-p)^{m+n-k} {m+n \choose k}
    \end{align*}
    $$
    Which is the PMF of $Bin(m+n, p)$ with $k$ successes.


### Hypergeometric Distribution

We have $b$ black and $w$ white, distinguishable marbles. We pick a random sample of size $n$ out of these marbles. The distribution of the number of white marbles $X$ in the sample is called the hypergeometric distribution and we say
$$
X \sim HGeom(w, b, n)
$$

**PMF**:
$$
P(X=k) = \frac{{w \choose k} {b \choose n-k}}{w+b \choose n}. \quad 0 \leq k \leq w, \quad 0 \leq n-k \leq b
$$

This PMF comes from sampling _without_ replacement. 
If we had replacement, it'd be reduced to the binomial distribution. (With probability $\sim$ #all white marbles.)

_Checking normalization_:
$$
\sum_{k=0}^w \frac{{w \choose k}{b \choose n-k}}{w+b \choose n} \stackrel{Vandermonde}{=} \frac{w+b \choose n}{w+b \choose n} = 1
$$
<hr/>

### Geometric Distribution
$Geom(p)$: Given indepdent $Bern(p)$ trials, count the $\# failures$ before the $1$st success.
_PMF_ (Consider $X \sim Geom(p)$):
$$
P(X=k) = (1-p)^k p, \quad k = {0, 1, 2, ...}
$$
_Intuition_:
All of the first $k$ trials fail with probability $1-p$, then the last one succeeds with probability $p$.

The PMF is valid since (normalization check):
$$
\begin{align*}
\sum_{k=0}^{\infty} p (1-p)^k &= p \sum_{k=0}^{\infty} (1-p)^k \\
&= \frac{p}{1 - (1-p)} \\
&= 1
\end{align*}
$$

_Note_ that the last equality comes from the $\infty$-sum of a **geometric** series:
$$
\sum_{k=0}^{\infty} q^k = \frac{1}{1-q}
$$

### Properties of CDF:
Remember, CDF: $F(x) = P(X \leq x)$

1. (non-strictly) Increasing
2. (approaching from) Right-continuous.
3. $$ \lim_{x \rightarrow -\infty} F(x) = 0, \quad \lim_{x \rightarrow \infty} F(x) = 1 $$ 

Iff a function has all of these properties, it is a CDF.

### Independence of Random Variables
$X, Y$ are independent random variables if:
$$
P(X \leq x, Y \leq y) = P(X \leq x)P(Y \leq y)
$$
for all $x, y$.

In the discrete case the condition becomes:
$$
P(X = x, Y = y) = P(X = x)P(Y = y)
$$

