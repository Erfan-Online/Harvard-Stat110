



























## Variance of Hypergeometric Distribution

We want to calculate the variance of a hypergeometric distribution. Which has the parameters $w$ white balls, $b$ black balls, and we're taking a sample of size $n$ without replacement. Then we want to study the variance of the number of white balls in the sample.

Let's define $p = \frac{w}{w + b}$, and denote $N = w + b$. 

We also define random variables $X_j$ being indicator random variables for the $j$-th ball being white. (Gives $1$ if white, $0$ if black.) \
Then we can write $X = \sum_{j=1}^n X_j$ as the number of white balls in the sample. \
From the previous lecture, we know that the variance of sums is the sum of variances, plus the covariance terms. \

$$
\begin{align*} 
Var(\sum_{j=1}^{n} X_j) 
&= Var(X_1) + Var(X_2) + \dots + Var(X_n) + 2 \sum_{i < j} Cov(X_i, X_j) \\
&= n \times Var(X_1) + 2 \times {n \choose 2} Cov(X_1, X_2) \\
&= n \times p \times (1-p) + 2 \times {n \choose 2} \Big( \frac{w}{w+b} \times \frac{w-1}{w + b - 1} - p^2 \Big) \\
&= \frac{N-n}{N-1} \times n \times p \times (1-p)
\end{align*}
$$

Note that these are all unconditional variances, because before you draw any ball, the $j$-th ball, $X_j$, is just equally likely to be any of the balls; so by symmetry they're all the same. Again by symmetry, the covariance terms between any two balls is the same.

For the first term in the sum in the $2$nd line, we have $n$ terms with the same equivalence, each of which is just a Bernoulli random variable with parameter $p$. So we have $n \times p \times (1-p)$. \
And for the second term, remember that $Cov(X_1, X_2) = E(X_1 X_2) - E(X_1) E(X_2)$.

As for the $3$rd line, as we said last time, if you multiply two indicator random variables, that's just an indicator random variable of the intersection. (Think about it in terms of set theory, it makes sense)

Let's take a close look at the final result: $\frac{N-n}{N-1} \times [n \times p \times (1-p)]$. 

The part in brackets is just the variance of a binomial random variable with parameters $n$ and $p$. \
The factor in front is called the **finite population correction factor**. (That's why I thought it's so similar toe the binomial!)

Again, let's use our good old trick of checking the simple and extreme cases.

If $n = 1$, then the variance is $p \times (1-p)$, which is the same as the variance of a _Bernoulli_ random variable, because why does it matter if you're sampling with replacement or not if you're only taking one sample? 

If $N >> n$, the correction factor will be extremely close to $1$, so the variance will be extremely close to $n \times p \times (1-p)$, which is the variance of a _binomial random_ variable. Which again makes sense because if the sample is small compared to the population, then it's highly unlikely that you'll draw the same ball twice, so yeah, you're not doing replacement, but what difference does it make?

## Tranformations (Change of Variables)

Previously, we used LOTUS a lot to get the expectation of a function of a random variable, but LOTUS doesn't give you the whole distribution. \
So how do you do that?

### Theorem (for the continuous case)

Let $X$ be a continuous random variable with PDF $f_x$, and let $Y = g(X)$. \
[Now we need to make some assumptions first, because if $g$ is some crazy function, then it's not going to work. LOTUS will still be true, but this theorem won't give us the distribution of $Y$] \
Assuming $g$ is differentiable and _strictly increasing_, then:

$$
f_{Y}(y) = f_{X}(x) \frac{dx}{dy}
$$

where $y = g(x)$, and then we have to _re-write the above equation in terms of $y$,_ using $x = g^{-1}(y)$. Also remember from calculus that $\frac{dx}{dy} = (\frac{dy}{dx})^{-1}$.

---

Sidenote: (Because I still love this theorem)

Assuming $f = g^{-1}$, then $g(f(x)) = x$. \
Taking the derivative of this, we get:

$$
f'(x) \times g'(f(x)) = 1 \Rightarrow f'(x) = \frac{1}{g'(f(x))}
$$

Or, writing $y = f(x), x = g(y)$ and using the Leibniz notation:

$$
f'(x) = \frac{dy}{dx}, \quad g'(f(x)) = \frac{d g(y)}{dy} = \frac{dx}{dy} \\
\Rightarrow \frac{dy}{dx} = (\frac{dx}{dy})^{-1 }
$$

---

**Proof**:

Let's find the CDF and take the derivative to get the PDF. The CDF is:

$$
\begin{align*} 
P(Y \leq y) 
&= P(g(X) \leq y) \\
&= P(X \leq g^{-1}(y)) \\ 
&= F_X(g^{-1}(y)) \\
&= F_X(x) \\
\, \\
\Rightarrow 
f_Y(y)
&= \frac{d}{dy} F_X(x) \\
&= f_X(x) \frac{dx}{dy} \\
\end{align*}
$$

(Note that because $g$ is strictly increasing, $g^{-1}$ is also strictly increasing, so the inequality doesn't flip. And the last line is just the chain rule.)


**Example**: 

$Y = e^{Z}, \quad Z \sim N(0,1)$ which is called a log-normal distribution. \
(This is increasing and infinitely differentiable, so we're good.) 

So writing down the CDF of $Y$, we get:

$$
\begin{align*} 
f_{Y}(y) 
&= f_{Z}(z) \frac{dz}{dy} \\ 
&= \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} (\ln y)^2} \times \frac{1}{y}
\end{align*}
$$


## Transformations in $\mathbb{R}^{n}$

Now we say $\vec{Y} = g(\vec{X})$ is a **Random Vector**, where $g: \mathbb{R}^{n} \mapsto \mathbb{R}^{m}$ and $\vec{X} = (X_1, X_2, \dots, X_n)$ is a continuous random vector which has some joint PDF $f_{\vec{X}}(\vec{x})$. \

Then the joint PDF of $\vec{Y}$ is:

$$
f_{\vec{Y}}(\vec{y}) = f_{\vec{X}}(\vec{x}) \Big| \frac{\partial \vec{x}}{\partial \vec{y}} \Big|
$$

Where the $\frac{\partial \vec{x}}{\partial \vec{y}}$ is the **Jacobian** of the transformation, which is the matrix of all the partial derivatives of the two vectors. (Then we take the _absolute value_ of the determinant of that matrix.)

$$
\begin{align*}
\frac{\partial \vec{x}}{\partial \vec{y}}
&=
\begin{pmatrix}
\frac{\partial x_1}{\partial y_1} & \frac{\partial x_1}{\partial y_2} & \dots & \frac{\partial x_1}{\partial y_m} \\
\, \\
\frac{\partial x_2}{\partial y_1} & \frac{\partial x_2}{\partial y_2} & \dots & \frac{\partial x_2}{\partial y_m} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial x_n}{\partial y_1} & \frac{\partial x_n}{\partial y_2} & \dots & \frac{\partial x_n}{\partial y_m} \\
\end{pmatrix} \\
\end{align*}
$$

Remember that even in this case, $|\frac{\partial \vec{x}}{\partial \vec{y}}| = |\frac{\partial \vec{y}}{\partial \vec{x}}|^{-1}$, you just have to think a little to figure out which direction to take the derivative in. (for easiest computation)


## Convolution (sums)

We want the distribution of a sum of random variables. 

Let $T = X + Y$, and we want to find the distribution of $T$ given the distributions of $X$ and $Y$ which are independent. \

In the discrete case, we can write:

$$
P(T = t) = \sum_{x} P(X = x, Y = t-x)
$$

(Pretty easy, right? The sum has to be $t$, so we just sum over all the possible values of $X$ and $Y$ that add up to $t$.) \

Now, for the continuous case, we want the PDF instead of the PMF, so we write:

$$
f_{T}(t) = \int_{-\infty}^{\infty} f_{X}(x) f_{Y}(t-x) dx
$$

We have not proven this, this is just an analogy to the discrete case, so we need some justification. The simplest way is to take the CDF:

$$
\begin{align*}
F_{T}(t) 
&= P(T \leq t) \\
&\stackrel{LOTP}{=} \int_{-\infty}^{\infty} P(X + Y \leq t | X=x) f_{X}(x) dx \\
&\stackrel{\text{Plug in } x}{=} \int_{-\infty}^{\infty} P(Y \leq t - x) f_{X}(x) dx \\
&= \int_{-\infty}^{\infty} F_{Y}(t-x) f_{X}(x) dx \\
\end{align*}
$$

Taking the derivative of both sides:

$$
f_{T}(t) = \int_{-\infty}^{\infty} f_{Y}(t-x) f_{X}(x) dx
$$


(Now we're going to talk about something beautiful; actually beautiful is not an adequate word to describe this, it's more _exisitential_.)

## Idea!

You can use probability to prove existence of objects with desired property $A$.

**Strategy** : Show $P(A) > 0$ for a random object. \
Note that we get to choose how to define random; that is we just have this universe of objects and we decide of some method for randomly selecting an object. \
So if it is a finite set, the most obvious way is to just pick one at random where they're all equally likely.

It should be clear that if the probability is non-zero, then there must exist an object with property $A$.

This sounds like very, very wishful thinking strategy. Like if we can't exhibit the existence of even one such object, how are we ever going to compute the probability? \
Notice that we don't have to exactly compute the probability, we just have to find a bound that shows it's greater than zero. 

**Example**: Suppose we have a set of objects, where each object has a "score". Now we want to show that there is an object with a "good" score.

We know that there is an object whose score is at least the average score $\mathbb{E}(X)$. So if we can show that $\mathbb{E}(X)$ is a "good" score, then we're done without having to find an object with a "good" score. 

You could say ok, but how is this ever useful?

### Shannon's Theorem

One of the most beautiful and useful theorems of the $20$th century was proved by Claude Shannon in $1948$. He is the father of information theory. This is not an information theory, but we will very briefly talk about it.

The theorem is about trying to communicate over a noisy channel, meaning that you're trying to send messages from one place to another, but bits get corrupted and there's a lot of noise and interference in the channel.

Shannon showed that there's something called the _Capacity_ of the channel, and you can communicate at rates arbitrarily close to the capacity of the channel, with arbitrarily small chance of error. Meaning that even if you have a very noisy channel, you can make the error probability very, very low. (No one else was even close to thinking about these things at his time)

The way is proved it is that there exists what he called a _good code_, which is a code that works well for sending messages across this noisy channel. And the way he showed the existence of this code was by picking a random code and showing that even the random one has the right properties.

That's the most daring thing you can imagine! He probably spend months trying to actually find one, couldn't find one, and then said "ok, let's just pick one at random and see what happens". And it worked! This is kind unbelievable, but it's true! It was only $30$ or $40$ years later that people actually found a way to construct a good code.


**Example**:  \
We have $100$ people, and those people form $15$ groups of $20$, and let's assume everyone is on $3$ groups. (If we have everyone in the same number of groups, then they would be in groups of size $3$.) \
Show that there _exists_ $2$ groups whose overlap is at least $3$.

**Idea**: Find the average overlap of $2$ random groups!

We're assuming that we have this fixed assignment of who's in what groups. (So on so is in group $1$, so on so is in group $2$, etc.) This is not random. Our randomness comes from the fact that we're choosing two groups at random.

How can we do that? _Indicator random variables_ for each person. \
So, we have chosen two groups at random, let's find the probability that they overlap by at least $3$. Using the fundamental bridge we have:

$$
\begin{align*} 
P(\text{number of overlaps} ) 
&= 100 \times P(\text{Person 1 on both randomly chosen groups}) \\
&= 100 \times \frac{3 \choose 2}{15 \choose 2} \\
&= 100 \times \frac{3}{15 \times 7} \\
&= \frac{20}{7} < 3
\end{align*}
$$

Nominator: Person $1$ is on $3$ groups, and we're choosing $2$ of them. \
Denominator: We're choosing $2$ groups out of the $15$ groups. \

We see that this is almost good enough, because we want the overlap to be at least $3$, but this implies there exists a pair of groups with at least an overlap of  $\geq \frac{20}{7}$. \

Now, there is no way that two groups can have an overlap equal to $\frac{20}{7}$, because the overlap has to be an integer. So we get to just round it up to the next integer which is $3$, which means that there _exists_ a pair of groups with an overlap of at least $3$ (!)
