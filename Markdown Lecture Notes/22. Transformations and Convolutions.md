



























## Variance of Hypergeometric Distribution

We want to calculate the variance of a hypergeometric distribution. Which has the parameters $w$ white balls, $b$ black balls, and we're taking a sample of size $n$ without replacement. Then we want to study the variance of the number of white balls in the sample.

Let's define $p = \frac{w}{w + b}$, and denote $N = w + b$. 

We also define random variables $X_j$ being indicator random variables for the $j$-th ball being white. (Gives $1$ if white, $0$ if black.) \
Then we can write $X = \sum_{j=1}^n X_j$ as the number of white balls in the sample. \
From the previous lecture, we know that the variance of sums is the sum of variances, plus the covariance terms. \

$$
\begin{align*} 
Var(\sum_{j=1}^{n} X_j) 
&= Var(X_1) + Var(X_2) + \dots + Var(X_n) + 2 \sum_{i < j} Cov(X_i, X_j) \\
&= n \times Var(X_1) + 2 \times {n \choose 2} Cov(X_1, X_2) \\
&= n \times p \times (1-p) + 2 \times {n \choose 2} \Big( \frac{w}{w+b} \times \frac{w-1}{w + b - 1} - p^2 \Big) \\
&= \frac{N-n}{N-1} \times n \times p \times (1-p)
\end{align*}
$$

Note that these are all unconditional variances, because before you draw any ball, the $j$-th ball, $X_j$, is just equally likely to be any of the balls; so by symmetry they're all the same. Again by symmetry, the covariance terms between any two balls is the same.

For the first term in the sum in the $2$nd line, we have $n$ terms with the same equivalence, each of which is just a Bernoulli random variable with parameter $p$. So we have $n \times p \times (1-p)$. \
And for the second term, remember that $Cov(X_1, X_2) = E(X_1 X_2) - E(X_1) E(X_2)$.

As for the $3$rd line, as we said last time, if you multiply two indicator random variables, that's just an indicator random variable of the intersection. (Think about it in terms of set theory, it makes sense)

Let's take a close look at the final result: $\frac{N-n}{N-1} \times [n \times p \times (1-p)]$. 

The part in brackets is just the variance of a binomial random variable with parameters $n$ and $p$. \
The factor in front is called the **finite population correction factor**. (That's why I thought it's so similar toe the binomial!)

Again, let's use our good old trick of checking the simple and extreme cases.

If $n = 1$, then the variance is $p \times (1-p)$, which is the same as the variance of a _Bernoulli_ random variable, because why does it matter if you're sampling with replacement or not if you're only taking one sample? 

If $N >> n$, the correction factor will be extremely close to $1$, so the variance will be extremely close to $n \times p \times (1-p)$, which is the variance of a _binomial random_ variable. Which again makes sense because if the sample is small compared to the population, then it's highly unlikely that you'll draw the same ball twice, so yeah, you're not doing replacement, but what difference does it make?

## Tranformations (Change of Variables)

Previously, we used LOTUS a lot to get the expectation of a function of a random variable, but LOTUS doesn't give you the whole distribution. \
So how do you do that?

### Theorem (for the continuous case)

Let $X$ be a continuous random variable with PDF $f_x$, and let $Y = g(X)$. \
[Now we need to make some assumptions first, because if $g$ is some crazy function, then it's not going to work. LOTUS will still be true, but this theorem won't give us the distribution of $Y$] \
Assuming $g$ is differentiable and _strictly increasing_, then:

$$
f_{Y}(y) = f_{X}(x) \frac{dx}{dy}
$$

where $y = g(x)$, and then we have to _re-write the above equation in terms of $y$,_ using $x = g^{-1}(y)$. Also remember from calculus that $\frac{dx}{dy} = (\frac{dy}{dx})^{-1}$.

---

Sidenote: (Because I still love this theorem)

Assuming $f = g^{-1}$, then $g(f(x)) = x$. \
Taking the derivative of this, we get:

$$
f'(x) \times g'(f(x)) = 1 \Rightarrow f'(x) = \frac{1}{g'(f(x))}
$$

Or, writing $y = f(x), x = g(y)$ and using the Leibniz notation:

$$
f'(x) = \frac{dy}{dx}, \quad g'(f(x)) = \frac{d g(y)}{dy} = \frac{dx}{dy} \\
\Rightarrow \frac{dy}{dx} = (\frac{dx}{dy})^{-1 }
$$

---

**Proof**:

Let's find the CDF and take the derivative to get the PDF. The CDF is:

$$
P(Y \leq y) = P(g(X) \leq y) = P(X \leq g^{-1}(y)) = F_X(g^{-1}(y))
$$

(Note that because $g$ is strictly increasing, $g^{-1}$ is also strictly increasing, so the inequality doesn't flip.)

[stopped at 14:47, will continue tomorrow]