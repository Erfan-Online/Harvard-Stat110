### Covariance
#### Definition:
$$
\begin{align*}
Cov(X, Y) &= \mathbb{E}\big((X - \mathbb{E}X) (Y - \mathbb{E}Y)\big) \\
&= \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y)
\end{align*}
$$
Where the second equality is easily proven by the linearity of expectation.

#### Properties:
1. 
$$
Cov(X, X) = Var(X)
$$

2.
$$
Cov(X, Y) = Cov(Y, X)
$$

3.
$$
Cov(X, c) = 0 \quad \text{if } c \text{ is a constant.}
$$

4.
$$
Cov(cX, Y) = c \times Cov(X, Y)
$$

5.
$$
Cov(X, Y + Z) = Cov(X, Y) + Cov(X, Z)
$$

(Properties $4$ and $5$ together are called _bi-linearity_, meaning that if we treat any of the coordinates as fixed, the other coordinate's behavior will look linear.)

6. 
$$
Cov(X + Y, Z + W) = Cov(X, Z) + Cov(X, W) + Cov(Y, Z) + Cov(Y, W) \\
Cov(\sum_{i=1}^m a_i X_i \, , \sum_{j=1}^n b_i Y_i) = \sum_{i, j} a_i b_j \times Cov(X_i, Y_j)
$$
Which follows immediately by using property $5$ repeatedly.

7. 
$$
\begin{align*}
Var(X_1 + X_2) &= Cov(X_1 + X_2, X_1 + X_2)  \\
&= Var(X_1) + Var(X_2) + 2 \times Cov(X_1, X_2)
\end{align*} 
\\ \\
Var(\sum_{i=1}^n X_i) = \sum_{i=1}^n Var(X_i) + 2 \times \sum_{i < j} Cov(X_i, X_j)
$$


**Theorem**:
If $X, Y$, are independent, then they're uncorrelated, i.e. $Cov(X, Y) = 0$.

_The converse isn't true in general!_
For example, consider $Z \sim N(0, 1), X = Z, Y = Z^2$

$$
\begin{align*}
Cov(X, Y) &= \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y) \\
&= \mathbb{E}(Z^3) - \mathbb{E}(Z)\mathbb{E}(Z^2) \\
&= 0
\end{align*}
$$
But they are very dependent! $Y$ is a function of $X$ and $Y$ determines the magnitude of $X$.

**Note**:
If every function of $X$ is uncorrelated with every function of $Y$, then they're independent.
(Covariance shows linear dependence.)

### Correlation
#### Definition
$$
\begin{align*}
Corr(X, Y) &= \frac{Cov(X, Y)}{SD(X) \times SD(Y)} \\
&= Cov(\frac{X - \mathbb{E}(X)}{SD(X)}, \frac{Y-\mathbb{E}(Y)}{SD(Y)})
\end{align*}
$$
Where $SD$ denotes standard deviation.

Correlation is making things _dimensionless_, so we won't have to deal with whether we're using nanometers or light-years. 
(For example, a covariance of $42$ is much more significant when we're using light-years.)
$$
(Variance) = unit^2 \\
(SD) = unit \\
(\frac{X - \mathbb{E}(X)}{SD}) = 1
$$
This is a form of _Standardization_, it takes the input from whatever mean and variance it already has to $mean = 0$ and $variance = 1$.

#### Theorem:
$$
-1 \leq Corr(X, Y) \leq 1
$$
This is a form of the Cauchy-Schwarz inequality.

_Proof_ (non-Cauchy-Schwarz):
Without loss of generality, assume $X, Y$ are standardized, and let $Cov(X, Y) = Corr(X, Y) = \rho $. 
(since it's normalized, correlation is just the same as covariance, mean is $0$ and variance is $1$.)
Using property $7$, we can show that:
$$
\begin{align*}
0 \leq Var(X + Y) &= Var(X) + Var(Y) + 2 \times Corr(X, Y) \\
&= 2 + 2\rho \\
0 \leq Var(X - Y) &= Var(X) + Var(Y) - 2 \times Corr(X, Y) \\
&= 2 - 2\rho \\
\Rightarrow -1 \leq \rho \leq 1
\end{align*}
$$

### Example (Covariance in a Multinomial)
Consider $(X_1, ..., X_k) \sim Mult(n, \vec{p})$,
Find the $Cov(X_i, X_j)$ for all $i, j$.

_Solution_:
If $i=j$, $Cov(X_i, X_j) = Var(X_i) = np_i(1-p_i)$
if $i \neq j$:
$$
\begin{align*}
Var(X_i + X_j) &= Var(X_i) + Var(X_j) + 2 \times Corr(X_i, X_j) \\
&= np_i(1-p_i) + np_j(1-p_j) + 2 \times c \\
\end{align*}
$$
Using the _lumping property_ of the multinomial, we can just think of $X_i + X_j$ as a single variable $X_l$ with the corresponding $p_l = p_i + p_j$. Therefore,

$$
\begin{align*}
Var(X_i + X_j) &= n(p_i + p_j)(1- (p_i + p_j)) \\
&= np_i(1-p_i) + np_j(1-p_j) + 2 \times c \\
\Rightarrow Cov(X_i, X_j) = -n p_i p_j
\end{align*}
$$
The $-$ sign makes a lot of sense, because the total number of events in a constant, and if one of the events starts happening more frequently, there will be a lower number of events to happen in all the other cases.

**Note**:
Let $I_A$ be the indicator random variable of event $A$. Then, 
$$
I_A^2 = I_A, \\ 
I_A^3 = I_A \\
I_A I_B = I_{A \cap B}
$$


#### Example (Binomial distribution)
Consider $X \sim Bin(n, p)$, write it as:
$$X = \sum_{i=1}^n X_i, \quad X_j \stackrel{iid}{\sim} Bern(p)$$.
Then we know 
$$Var(X_i) = \mathbb{E}(X_j^2) - (\mathbb{E}(X_j))^2 = p - p^2 = p(1-p)$$
Therefore, 
$$Var(X) = np(1-p)$$ since $Cov(X_i, X_j) = 0$ for $i \neq j$. (because they're iid)

#### Example (Hypergeometric distribution)
[Remember that the hypergeometric distribution is the distribution of the number of white balls in a container of $w$ white balls, $b$ black balls and $n$ queries without replacement.]

Consider $X \sim HGeom(w, b, n)$, we can write it as:
$$
X = X_1 + ... + X_n, \quad
X_j = 
\begin{cases}
1 \text{\quad if $j$-th ball is white } \\
0 \text{\quad otherwise}
\end{cases}
$$

But here, the $X_j$'s are _dependent_ indicator random variables, because it's _without_ replacement, which makes the problem a little harder. Therefore, we have to exploit the _symmetries_ of the problem. 

$$
Var(X) = n \times Var(X_1) + 2 \times {n \choose 2} Cov(X_1, X_2)
$$

_Why is the variance the same for all $X_i$?_
Because let's imagine the $7$-th ball, before we do anything, it's likely to be any of the balls in the container. Same for all the other balls. So, the variance is the same for all $X_i$ and the covariance is the same for all $X_i$ and $X_j$, the $n \choose 2$ term is because we have that amount of pairs in there.
Now, let's calculate the terms.

Note that the variance is just the variance of a bernoulli random variable, so we can use the same formula.

$$
\begin{align*}
Cov(X_1, X_2) &= \mathbb{E}(X_1 X_2) - \mathbb{E}(X_1) \mathbb{E}(X_2) \\
&= \frac{w}{w+b} \times \frac{w-1}{w+b-1} - (\frac{w}{w+b})^2
\end{align*}
$$

Because
$\mathbb{E}(X_1) = \mathbb{E}(X_2)$ is the probability of $X_1$ being white, which is the same for all $X_i$ before we do anything.
And
$\mathbb{E}(X_1 X_2)$ is the probability of $X_1$ and $X_2$ being white, because of the previously mentioned $I_A I_B = I_{A \cap B}$ fact.